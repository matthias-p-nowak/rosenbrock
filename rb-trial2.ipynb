{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abfa989-4f78-4140-9b15-324f2a060d53",
   "metadata": {},
   "source": [
    "# Rosenbrock function\n",
    "\n",
    "Rosenbrock function (2D):\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + b \\cdot (y - x^2)^2\n",
    "$$\n",
    "$$\n",
    "b = 100\n",
    "$$\n",
    "- Global minimum: $f(x, y) = 0$ at $(x, y) = (1, 1)$\n",
    "- Non-convex, commonly used to test optimization algorithms\n",
    "- badness parameter $b$\n",
    "\n",
    "### Aspects\n",
    "- starting point with large gradients\n",
    "- turning in the valley into the slow descent direction\n",
    "- gradient in one direction is a multitude larger than the other\n",
    "- progress in the valley\n",
    "- dealing with change of the curvature near $x=0$\n",
    "- approaching optimum\n",
    "- convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88d431-3aa7-4751-ae0c-e8de1d575810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import RK45\n",
    "import adambash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9ef69-7d96-4802-8c05-da2f682032c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock function\n",
    "def rosenbrock(x, y, a=1, b=100):\n",
    "    return (a - x)**2 + b*(y - x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4f75e-71fc-46f8-b1df-d686a6d70eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial point and optimization\n",
    "x0 = np.array([-1, 2])\n",
    "# Plot the Rosenbrock function and path\n",
    "x = np.linspace(-2, 2, 400)\n",
    "y = np.linspace(-1, 3, 400)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23d8be-67ca-406f-a3b3-f4c7584950cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepPlot():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    levels = np.array([i**2 for i in range(21)])\n",
    "    plt.contour(X, Y, Z, levels=levels, cmap='rainbow')\n",
    "    plt.plot(1, 1, 'bo', label='Minimum (1,1)')\n",
    "    plt.plot(-1,2,'co',label='Start')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Rosenbrock Function')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf092b-1ed8-4748-a20a-f8caa93547ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepPlot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19063e93-e9da-4d85-88ca-b515abb55f43",
   "metadata": {},
   "source": [
    "- it's a valley: blue=down, red=high up\n",
    "- Left green: starting point with descent direction down left $f(x,y)=104$\n",
    "- Right blue: optimum in a long valley - $f(x,y)= 0$\n",
    "- level lines (deformed circles) at $ 0,1,4,9,..,400$\n",
    "- a line between the 2 points goes over a hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd8e99-f9d9-4525-9ba8-6507597ceec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the Rosenbrock function\n",
    "def grad_rosenbrock(xy, a=1, b=100):\n",
    "    x, y = xy\n",
    "    dx = 2*(x-a) - 4*b*x*(y - x**2)\n",
    "    dy = 2*b*(y - x**2)\n",
    "    return np.array([dx, dy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5612e-8f21-43ba-8d7b-36e19c9432e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steepest descent method\n",
    "def steepest_descent(f_grad, x0, lr=0.001, max_iter=100, tol=1e-6):\n",
    "    x = x0\n",
    "    path = [x0.copy()]\n",
    "    for i in range(max_iter):\n",
    "        grad = f_grad(x)\n",
    "        x_new = x - lr * grad\n",
    "        if np.linalg.norm(x_new) > 20:\n",
    "            break\n",
    "        path.append(x_new.copy())\n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            break\n",
    "        x = x_new\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5596b-4e76-49fc-931c-dca875e158c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepPlot()\n",
    "path = steepest_descent(grad_rosenbrock, x0, lr=0.0017, max_iter=100)\n",
    "plt.plot(path[:, 0], path[:, 1], 'r.-', label='calculated')\n",
    "plt.title('gradient descent, lr=0.0017, 100 iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66362cf-4021-4e43-b953-fb2d9edeee59",
   "metadata": {},
   "source": [
    "### common problem\n",
    "- stagnation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf9e30-72a1-407f-b424-fc851d98ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepPlot()\n",
    "path = steepest_descent(grad_rosenbrock, x0, lr=0.0021, max_iter=100)\n",
    "plt.plot(path[:, 0], path[:, 1], 'r.-', label='calculated')\n",
    "plt.title('gradient descent, lr=0.0021, aborted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67759d-a143-4371-8ee5-976a2eef7bf2",
   "metadata": {},
   "source": [
    "### common problem\n",
    "- starting outside the convergence zone\n",
    "- problematic when function contains products of variables, $x_1 \\cdot x_2 \\cdot x_3$ &rarr; *exploding gradients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f411956-bf76-47dd-8cfd-23765ebf0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepPlot()\n",
    "path = steepest_descent(grad_rosenbrock, x0, lr=0.0019, max_iter=10)\n",
    "plt.plot(path[:, 0], path[:, 1], 'r.-', label='calculated')\n",
    "plt.title('gradient descent, lucky lr=0.00191 , 10 iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e93be-0d11-45a6-95c6-1887e2851581",
   "metadata": {},
   "source": [
    "## Gradient flow\n",
    "\n",
    "- gradient descent is like *explicit Euler integration*, why not take it further?\n",
    "- need to change x according to the experienced gradient\n",
    "$$ \\dot{z} = g(t, z), z=(x,y) $$\n",
    "\n",
    "#### Python code for gradienten:\n",
    "~~~\n",
    "dfdx = -2*(a - x) - 4*b*x*(y - x**2)\n",
    "dfdy = 2*b*(y - x**2)\n",
    "~~~\n",
    "#### why it may work\n",
    "- can find the optimum by integrating to infinity\n",
    "- integration method Runge-Kutta, with additional time parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea471e-586a-4dde-8663-b93f95bbd986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradFlow(t,xy):\n",
    "    xy_bar = -1 * grad_rosenbrock(xy, 1, 100)\n",
    "    return xy_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1b3a9-df2f-4a1e-909e-7d7401168caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RKLpath(max_iter=10):\n",
    "    rk45 = RK45(gradFlow,0,x0.copy(), 10,max_step=20000)\n",
    "    path=[x0.copy()]\n",
    "    for s in range(max_iter):\n",
    "        rk45.step()\n",
    "        path.append(rk45.y.copy())\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b5016-8ad9-4dd7-8622-19840874294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepPlot()\n",
    "# Solve using Runge-Kutta 4(5) (RK45)\n",
    "path = RKLpath(1000)\n",
    "plt.plot(path[:, 0], path[:, 1], 'r.-', label='calculated')\n",
    "plt.title(\"Runge-Kutta 4(5), 1000 iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab4e07-58ab-439f-83c1-bac855dadc6c",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Each Runge Kutta step has 4 function evaluations at half stepsize and multiple estimation step.\n",
    "- it is costly\n",
    "- tries to find a monotonically decreasing path &rarr; hence has to stay at the bottom of the valley\n",
    "- zigzagging with small steps through the valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed3546-7de3-4f31-a3ba-606965d92411",
   "metadata": {},
   "source": [
    "### Explicit higher order methods\n",
    "- Adam Bashfort schemes - up to order 6\n",
    "- considers a polynomial approximation of the gradients\n",
    "- not bound to decreasing objective values\n",
    "- constant stepsize (not update size!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963c9ca-b40f-43b1-b847-7fdfaf9c3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepPlot()\n",
    "ab= adambash.AdamBashfort(grad_rosenbrock, x0.copy(),0.0005)\n",
    "path = ab.iterate(max_order=3, max_iter=1000 )\n",
    "plt.plot(path[:, 0], path[:, 1], \"r.-\", label=\"calculated\")\n",
    "plt.title(\"Adam Bashfort 3rd order, 1000 iterations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0594f8-bbe9-4960-8e6f-ef5a2b8c818b",
   "metadata": {},
   "source": [
    "### From previous work\n",
    "- a method that works without explicit step length\n",
    "- uses memory - the tail of recent steps &rarr; calculating next step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f427ce-d242-47fb-90ef-47d52e6e1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import memgradstep\n",
    "prepPlot()\n",
    "mgs=memgradstep.MemGradStep(grad_rosenbrock,x0.copy(),decay=0.5,memory=10)\n",
    "path,obs =mgs.iterate(1000)\n",
    "plt.plot(path[:,0],path[:,1],\"r.-\",label=\"calculated\")\n",
    "plt.title(\"MemGradStep,1000 iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc026b-d360-4cfd-82a6-c911418d3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(obs, \"r.-\", label=\"step size\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Step sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7970b3-8141-45c2-a9b4-443c2a75cc52",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- startup works really well, increasing step sizes up to level needed\n",
    "- step size adjustment works\n",
    "- too much zigzagging &rarr; too slow\n",
    "- works very well for small *badness parameter values* $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84186f90-bad1-4685-ba4c-f5887861674d",
   "metadata": {},
   "source": [
    "## How about Adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a2089-fb09-4c85-b6a6-1cd44691cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adamtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e798c6-dbdc-4756-82ab-555b0d256f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepPlot()\n",
    "# function, gradient, start point, step length, beta_1, beta_2, epsilon(stopping)\n",
    "a = adamtest.Adam(rosenbrock, grad_rosenbrock, x0.copy(), 0.3, 0.9, 0.99, 1e-8)\n",
    "obs, path, vh, mh = a.iterate(1000)\n",
    "plt.plot(path[:, 0], path[:, 1], \"r.-\", label=\"calculated\")\n",
    "plt.legend()\n",
    "plt.title(\"Adam, 1000 iterations\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeeb04c-6ee0-428f-a32b-19fbb785f3a6",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- stepping down really fast, overshooting\n",
    "- zigzagging and getting speed\n",
    "- ok through the valley and curvature change\n",
    "- trouble converging\n",
    "- **same stepsize** all along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e061d-91f7-4882-8fc9-fe8c80f9fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(obs[:, 0], \"r.-\", label=\"objective\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Objective function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc6e47-1a72-4c2e-9572-9e7ad7db31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(obs[:, 1], \"b.-\", label=\"gradient\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e576e0e-f0f7-4952-9f63-aee80d87198e",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- no valid stopping criterium\n",
    "- momentum leads to overshooting\n",
    "- needs scheduled learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bac04e-1bab-45ac-8444-68bcaf8a8210",
   "metadata": {},
   "source": [
    "# Scheduling\n",
    "- decent into valley &rarr; small step sizes to avoid overshooting due to large gradients\n",
    "- slowly increaing step sizes to move into right direction\n",
    "- decay at the end to let the steps circle around optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0d574-3b55-4a2a-aff6-27eadb12d43a",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent and batches\n",
    "\n",
    "- $f(x), \\frac{f(x)}{dx} \\Rightarrow f_{\\xi}(x), \\frac{f_{\\xi}(x)}{dx}$  going from deterministic functions to stochastic ones\n",
    "- $f_{\\xi}(x) = f(x)+\\epsilon $, i.e. the deterministic function but with a small stochastic noise $\\epsilon$\n",
    "- each gradient over a full epoch &rarr; $\\epsilon=0$\n",
    "- smaller versus larger batches &rarr; $\\epsilon_{small} > \\epsilon_{large}$: the larger the batch, the smaller the noise + the better the gradients\n",
    "\n",
    "## Random sampling\n",
    "- avoids systematic errors and overfitting\n",
    "- makes the expected mean small\n",
    "- avoids cycling around the optimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06fc5f-6407-45db-a1a2-db90cf645599",
   "metadata": {},
   "source": [
    "# Future \"work\" and research\n",
    "- take the knowledge from *memory adjusted step size* further\n",
    "- employ gradient rotation to smooth out zigzagging\n",
    "- limiting *path following* to stay below a treshhold (avoid exploding gradients)\n",
    "    - *path following* from Guddat et.al.: Parametric Optimization: Singularities, Pathfollowing and Jumps\n",
    "- test on \"stochastic Rosenbrock\" to simulate the stochastic batches\n",
    "- test on AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7676141-a4b8-4800-926b-cefd7dfdcfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
